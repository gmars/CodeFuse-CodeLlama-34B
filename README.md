# Model Card for CodeFuse-CodeLlama-34B
<p align="left">
  <img src="./LOGO.png" width="100%" />
</p>

[[ä¸­æ–‡]](#chinese)    [[English]](#english)



<a id="english"></a>

## Model Description

CodeFuse-CodeLlama-34B is a 34B Code-LLM finetuned by QLoRA of multiple code tasksï¼ˆ600k instrunctions/answersï¼‰ on the base model CodeLlama-34b-Python. 
The context length of finetuning is 4K while it is able to be finetuned by 16k context if necessary.
<br>

## News and Updates

ğŸ”¥ğŸ”¥ğŸ”¥ CodeFuse-CodeLlama34B-MFT has achived 74.4% of pass@1 on HumanEval, which is SOTA at present.

<br>

## Performance

| Model                         | HumanEval(pass@1) |
| :---------------------------- | :---------------: |
| CodeLlama-34b                 |   48.8%(greedy decoding)   |
| CodeLlama-34b-Python          |   53.7%(greedy decoding)   |
| **CodeFuse-CodeLlama-34B** | **74.4%**(greedy decoding) |

<br>

## Requirements

* python>=3.8 
* pytorch>=2.0.0
* transformers==4.32.0
* Sentencepiece
* CUDA 11.4
  <br>

##  Inference String Format

The inference string is a concatenated string formed by combining conversation data(system, human and bot contents) in the training data format.  It is used as input during the inference process.
Here is an example format of the concatenated string:

```python
"""
<|role_start|>system<|role_end|>System instruction
<|role_start|>human<|role_end|>Human 1st round input
<|role_start|>bot<|role_end|>Bot 1st round output</s>
<|role_start|>human<|role_end|>Human 2nd round input
<|role_start|>bot<|role_end|>Bot 2nd round output</s>
...
...
...
<|role_start|>human<|role_end|>Human nth round input
<|role_start|>bot<|role_end|>{Bot output to be genreated}</s>
"""
```

When applying inference, you always make your input string end with "<|role_start|>bot<|role_end|>" to ask the model generating answers.

## Quickstart

```bash
pip install -r requirements.txt
```

```python
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
)
tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)
tokenizer.padding_side = "left"
tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<unk>")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("</s>")
model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, trust_remote_code=True)

HUMAN_ROLE_START_TAG = "<|role_start|>human<|role_end|>"
BOT_ROLE_START_TAG = "<|role_start|>bot<|role_end|>"

text = f"{HUMAN_ROLE_START_TAG}write a python function of quick sort.{BOT_ROLE_START_TAG}" 
inputs = tokenizer(text, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
```













<a id="chinese"></a>

## æ¨¡å‹ç®€ä»‹

CodeFuse-CodeLlama34B-MFT æ˜¯ä¸€ä¸ªé€šè¿‡QLoRAå¯¹åŸºåº§æ¨¡å‹CodeLlama-34b-Pythonè¿›è¡Œå¤šä»£ç ä»»åŠ¡å¾®è°ƒçš„ä»£ç å¤§æ¨¡å‹ã€‚æ¨¡å‹å¾®è°ƒé‡‡ç”¨äº†4kä¸Šä¸‹æ–‡ã€‚å¦‚æœæœ‰å¿…è¦ï¼Œå¯ä»¥æ‰©å±•åˆ°16kã€‚
<br>

## æ–°é—»

ğŸ”¥ğŸ”¥ğŸ”¥ CodeFuse-CodeLlama34B-MFTæ¨¡å‹åœ¨HumanEval pass@1ä¸Šå¯ä»¥è¾¾åˆ°74.4%, ä¸ºå½“å‰å¼€æºSOTAã€‚

<br>

## è¯„æµ‹è¡¨ç°(ä»£ç )

| æ¨¡å‹                         | HumanEval(pass@1) |
| :---------------------------- | :---------------: |
| CodeLlama-34b                 |   48.8%(greedy decoding)   |
| CodeLlama-34b-Python          |   53.7%(greedy decoding)   |
| **CodeFuse-CodeLlama-34B** | **74.4%**(greedy decoding) |
<br>

## Requirements

* python>=3.8 
* pytorch>=2.0.0
* transformers==4.32.0
* CUDA 11.4
<br>

## æ¨ç†æ•°æ®æ ¼å¼

æ¨ç†æ•°æ®ä¸ºæ¨¡å‹åœ¨è®­ç»ƒæ•°æ®æ ¼å¼ä¸‹æ‹¼æ¥çš„å­—ç¬¦ä¸²å½¢å¼ï¼Œå®ƒä¹Ÿæ˜¯æ¨ç†æ—¶è¾“å…¥promptæ‹¼æ¥çš„æ–¹å¼ï¼š

```python
"""
<|role_start|>system<|role_end|>è¿™æ˜¯SystemæŒ‡ä»¤
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬1è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>è¿™æ˜¯ç¬¬1è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹</s>
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬2è½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>è¿™æ˜¯ç¬¬2è½®æ¨¡å‹ç”Ÿæˆçš„å†…å®¹</s>
...
...
...
<|role_start|>human<|role_end|>è¿™æ˜¯ç¬¬nè½®ç”¨æˆ·è¾“å…¥çš„é—®é¢˜
<|role_start|>bot<|role_end|>{æ¨¡å‹ç°åœ¨è¦ç”Ÿæˆçš„å†…å®¹}</s>
"""
```

æ¨ç†æ—¶ï¼Œè¯·ç¡®ä¿æ‹¼æ¥çš„promptå­—ç¬¦ä¸²ä»¥"<|role_start|>bot<|role_end|>"ç»“å°¾ï¼Œå¼•å¯¼æ¨¡å‹ç”Ÿæˆå›ç­”ã€‚

## å¿«é€Ÿä½¿ç”¨

```python
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
)
tokenizer = AutoTokenizer.from_pretrained(mode_name_or_path, trust_remote_code=True, use_fast=False, legacy=False)
tokenizer.padding_side = "left"
tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids("<unk>")
tokenizer.eos_token_id = tokenizer.convert_tokens_to_ids("</s>")
model = AutoModelForCausalLM.from_pretrained(mode_name_or_path, trust_remote_code=True)

HUMAN_ROLE_START_TAG = "<|role_start|>human<|role_end|>"
BOT_ROLE_START_TAG = "<|role_start|>bot<|role_end|>"

text = f"{HUMAN_ROLE_START_TAG}write a python function of quick sort.{BOT_ROLE_START_TAG}" 
inputs = tokenizer(text, return_tensors='pt', padding=True, add_special_tokens=False).to("cuda")
outputs = model.generate(
        inputs=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=512,
        top_p=0.95,
        temperature=0.1,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.pad_token_id
    )
gen_text = tokenizer.batch_decode(outputs[:, inputs["input_ids"].shape[1]:], skip_special_tokens=True)
print(gen_text)
```


